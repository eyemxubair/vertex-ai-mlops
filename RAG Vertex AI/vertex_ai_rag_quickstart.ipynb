{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Vertex AI RAG \u2014 Beginner Quickstart (Project: `xubisid-demo3`)\n", "\n", "This notebook shows a **simple Retrieval-Augmented Generation (RAG)** pipeline using:\n", "\n", "- **Vertex AI Gemini** for generation\n", "- **Vertex AI `text-embedding-004`** for embeddings\n", "- **FAISS (in-memory)** for vector search (beginner-friendly)\n", "\n", "You can later swap FAISS for **Vertex AI Vector Search** once you're comfortable."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 0) Prerequisites (run in your terminal / Cloud Shell **once**)\n", "\n", "```bash\n", "export PROJECT_ID=xubisid-demo3\n", "export REGION=us-central1\n", "gcloud config set project $PROJECT_ID\n", "gcloud config set ai/region $REGION\n", "\n", "# Enable APIs\n", "gcloud services enable aiplatform.googleapis.com storage.googleapis.com run.googleapis.com\n", "\n", "# (Optional) Create a service account for notebooks / CI\n", "gcloud iam service-accounts create rag-sa --display-name=\"RAG Service Account\"\n", "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n", "  --member=\"serviceAccount:rag-sa@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n", "  --role=\"roles/aiplatform.user\"\n", "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n", "  --member=\"serviceAccount:rag-sa@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n", "  --role=\"roles/storage.objectAdmin\"\n", "\n", "# (Optional) Create a bucket for your datasets/artifacts\n", "export BUCKET=${PROJECT_ID}-rag-demo\n", "gsutil mb -l $REGION gs://$BUCKET || echo \"Bucket may already exist\"\n", "```\n", "\n", "Then open this notebook in **Vertex AI Workbench** or **Colab**. If using Workbench, ensure the runtime has internet and you are authenticated."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1) Install dependencies (only first time per runtime)"]}, {"cell_type": "code", "metadata": {}, "source": ["!pip -q install google-cloud-aiplatform>=1.60.0 faiss-cpu langchain>=0.2.0 langchain-community>=0.2.0 langchain-google-vertexai>=2.0.0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2) Set your project and region"]}, {"cell_type": "code", "metadata": {}, "source": ["PROJECT_ID = \"xubisid-demo3\"  # <- change if needed\n", "REGION = \"us-central1\"\n", "\n", "from google.cloud import aiplatform\n", "aiplatform.init(project=PROJECT_ID, location=REGION)\n", "print(\"Vertex AI initialized:\", PROJECT_ID, REGION)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) Prepare some sample documents\n", "We'll create a few small knowledge base documents directly in the notebook. Replace these with your PDFs/CSVs later."]}, {"cell_type": "code", "metadata": {}, "source": ["sample_docs = [\n", "    (\"doc1.txt\", \"GoBazzar is an e-commerce price comparison platform. It ingests product feeds and normalizes data.\"),\n", "    (\"doc2.txt\", \"EJADH Inspector app assigns field inspectors by region and city with shift-based scheduling.\"),\n", "    (\"doc3.txt\", \"Vertex AI supports Gemini for generation, and text-embedding-004 for high-quality embeddings.\"),\n", "]\n", "\n", "import os\n", "os.makedirs(\"data\", exist_ok=True)\n", "for fname, text in sample_docs:\n", "    with open(os.path.join(\"data\", fname), \"w\", encoding=\"utf-8\") as f:\n", "        f.write(text)\n", "print(\"Wrote\", os.listdir(\"data\"))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4) Chunk + Embed with Vertex AI `text-embedding-004`"]}, {"cell_type": "code", "metadata": {}, "source": ["from langchain_community.document_loaders import TextLoader\n", "from langchain.text_splitter import RecursiveCharacterTextSplitter\n", "from langchain_google_vertexai import VertexAIEmbeddings\n", "\n", "# Load docs\n", "docs = []\n", "for fname, _ in sample_docs:\n", "    loader = TextLoader(os.path.join(\"data\", fname), encoding=\"utf-8\")\n", "    docs.extend(loader.load())\n", "\n", "# Chunk\n", "splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=80)\n", "chunks = splitter.split_documents(docs)\n", "print(\"Total chunks:\", len(chunks))\n", "\n", "# Embeddings via Vertex AI\n", "emb_model = VertexAIEmbeddings(model_name=\"text-embedding-004\")\n", "embeddings = emb_model.embed_documents([c.page_content for c in chunks])\n", "print(\"Got\", len(embeddings), \"embeddings, dim:\", len(embeddings[0]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5) Build a FAISS vector index (local, simple)"]}, {"cell_type": "code", "metadata": {}, "source": ["import faiss\n", "import numpy as np\n", "\n", "dim = len(embeddings[0])\n", "index = faiss.IndexFlatIP(dim)  # cosine if normalized; here using dot-product\n", "vecs = np.array(embeddings).astype(\"float32\")\n", "faiss.normalize_L2(vecs)  # normalize for cosine-like similarity\n", "index.add(vecs)\n", "print(\"FAISS index size:\", index.ntotal)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6) Define a retriever helper"]}, {"cell_type": "code", "metadata": {}, "source": ["def retrieve(query: str, top_k: int = 3):\n", "    q_emb = emb_model.embed_query(query)\n", "    q = np.array([q_emb]).astype(\"float32\")\n", "    faiss.normalize_L2(q)\n", "    distances, idxs = index.search(q, top_k)\n", "    ctx = []\n", "    for i in idxs[0]:\n", "        if i == -1:\n", "            continue\n", "        ctx.append(chunks[i].page_content)\n", "    return ctx\n", "\n", "print(retrieve(\"What is GoBazzar?\"))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7) Call Gemini to generate grounded answers"]}, {"cell_type": "code", "metadata": {}, "source": ["from vertexai.preview.generative_models import GenerativeModel\n", "\n", "gemini = GenerativeModel(\"gemini-1.5-flash\")  # or \"gemini-1.5-pro\"\n", "\n", "def rag_answer(question: str, top_k: int = 3):\n", "    context = retrieve(question, top_k=top_k)\n", "    prompt = (\n", "        \"You are a helpful assistant. Use ONLY the context below to answer.\\n\\n\"\n", "        f\"Context:\\n{chr(10).join([f'- {c}' for c in context])}\\n\\n\"\n", "        f\"Question: {question}\\n\"\n", "        \"If the answer is not in the context, say you don't know.\"\n", "    )\n", "    resp = gemini.generate_content(prompt)\n", "    return resp.text\n", "\n", "print(rag_answer(\"How does EJADH Inspector app schedule inspectors?\"))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8) Try your own questions"]}, {"cell_type": "code", "metadata": {}, "source": ["questions = [\n", "    \"What models does Vertex AI provide for embeddings?\",\n", "    \"Explain GoBazzar in one sentence.\",\n", "]\n", "for q in questions:\n", "    print(\"\\nQ:\", q)\n", "    print(\"A:\", rag_answer(q))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 9) (Optional) Save/Load FAISS index\n", "You can persist the FAISS index to reuse without recomputing."]}, {"cell_type": "code", "metadata": {}, "source": ["faiss.write_index(index, \"faiss.index\")\n", "with open(\"chunks.txt\", \"w\", encoding=\"utf-8\") as f:\n", "    for c in chunks:\n", "        f.write(c.page_content.replace(\"\\n\", \" \") + \"\\n\")\n", "print(\"Saved faiss.index and chunks.txt\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 10) Next steps: Move to **Vertex AI Vector Search**\n", "Once you're ready, you can replace FAISS with **Vertex AI Vector Search**. The high-level steps:\n", "\n", "1. Create an index (Matching Engine / Vector Search) in Vertex AI\n", "2. Upload your embeddings to Cloud Storage as TFRecords or JSONL\n", "3. Deploy the index to an index endpoint\n", "4. Query the endpoint for the top-K vectors\n", "5. Feed retrieved chunks to Gemini as in `rag_answer`\n", "\n", "This gives you a managed, scalable vector store."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}